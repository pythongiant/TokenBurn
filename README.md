# ðŸ”¥ TokenBurn

## ðŸ“š References & Inspiration

This library was inspired by recent research on log-probabilities and Bayesian interpretations of LLMs:

* **â€œLLMs are Bayesian, in Expectation, not in Realizationâ€**
  [arXiv:2507.11768](https://arxiv.org/pdf/2507.11768)

These ideas motivated the **hallucination risk scoring**, **information-theoretic metrics**, and **optimal CoT length scaling** in TokenBurn.

---

**TokenBurn** is a lightweight Python library for analyzing the confidence of Large Language Models (LLMs).
It helps you detect **hallucinations**, compute **perplexity**, and find the **optimal chain-of-thought (CoT) length** using scaling laws + information-theoretic metrics.

It works with any Hugging Face / OpenAI-compatible inference server that supports logprobs.

---

## ðŸš€ Features

* âœ… Extract **logprobs** from LLM outputs
* âœ… Compute **perplexity** (model confidence)
* âœ… Detect **hallucination risk** using entropy + variance
* âœ… Estimate **optimal CoT length** with scaling law
* âœ… Compute **information-theoretic metrics** (Entropy, KL Divergence, Mutual Information)
* âœ… Compatible with Hugging Face Inference API / OpenAI-like endpoints

---

## ðŸ“¦ Installation

```bash
git clone https://github.com/yourname/tokenburn.git
cd tokenburn
pip install -r requirements.txt
```

---

## âš¡ Quickstart

```python
from tokenburn.tokenburn import TokenBurn

tb = TokenBurn(
    url="https://router.huggingface.co/v1/chat/completions",
    model="openai/gpt-oss-120b:cerebras",
    api_key="hf_xxxxx"
)

logprobs = tb.get_logprobs(messages=[
    {"role": "user", "content": "Who won the Nobel Prize in Physics in 2029?"}
], max_tokens=50, top_logprobs=5)

print("Perplexity:", tb.perplexity(logprobs))
print("Hallucination Risk:", tb.hallucination_risk(logprobs))
print("Optimal CoT Length:", tb.find_optimal_cot_length(n=len(logprobs), epsilon=0.9))
print("Entropy:", tb.entropy(logprobs))
print("KL Divergence vs. uniform:", tb.kl_divergence(logprobs, baseline="uniform"))
```

---

## ðŸ“Š Metrics Explained

### ðŸ”¹ Perplexity

* Confidence score from token logprobs:

$$
\text{PPL} = \exp\Bigg(-\frac{1}{N} \sum_{i=1}^N \log p(x_i)\Bigg)
$$

* **Low (â‰ˆ1â€“20)** â†’ confident prediction
* **High (>50)** â†’ uncertain, likely hallucination

---

### ðŸ”¹ Hallucination Risk

* Combines **entropy** (spread of probability distribution) + **variance** (instability of logprobs)
* Risk levels: **LOW / MEDIUM / HIGH**

---

### ðŸ”¹ Optimal CoT Length

* Scaling law for reasoning length:

$$
k^* \approx \sqrt{\frac{\alpha n}{H_\text{cot} (B_0 - B_\text{opt})}} \cdot \log_2\Big(\frac{1}{\epsilon}\Big)
$$

* Predicts how many reasoning steps are optimal before diminishing returns

---

### ðŸ”¹ Information-Theoretic Metrics

* **Entropy (H)** â€“ Uncertainty of the model:

$$
H(p) = -\sum_x p(x) \log p(x)
$$

* **KL Divergence**  â€“ How much the modelâ€™s distribution deviates from a baseline:

$$
D_\text{KL}(p || q) = \sum_x p(x) \log \frac{p(x)}{q(x)}
$$

* **Mutual Information (I)** â€“ Measures how much knowing the context reduces uncertainty about the next token:

$$
I(X;Y) = H(X) - H(X|Y)
$$

These metrics give a **principled, information-theoretic view** of model confidence and hallucination risk.

---

## ðŸ“‚ Project Structure

```
tokenburn/
â”‚â”€â”€ tokenburn.py         # Main TokenBurn class
â”‚â”€â”€ utils/
â”‚    â””â”€â”€ metrics.py      # Perplexity, entropy, KL divergence, CoT length, etc.
```

---

## âœ… Example Use Cases

* Detect when your LLM is **guessing vs. confident**
* Use entropy & KL divergence for **early hallucination detection**
* Benchmark models on **information-theoretic efficiency**
* Tune reasoning dynamically with **optimal CoT scaling**

---

## ðŸ›  Roadmap

* [ ] Add visualization for perplexity & entropy
* [ ] Add mutual information trend plots
* [ ] Streaming logprobs support
* [ ] Benchmark suite with hallucination prompts

---

## ðŸ“œ License

MIT License â€“ free to use and modify

---

